from heapq import heappush, heappushpop
from typing import List, Tuple
import torch
import torch.nn.functional as F
from tokens import EOS, Tokenizer
from robust_fill import RobustFill


def beam_search(
        model: RobustFill,
        tokenizer: Tokenizer,
        width: int,
        max_program_length: int,
        strings: List[Tuple[List[int], List[int]]]) -> List[Tuple]:
    """
    Beam search for the best program given a list of input-output pairs.

    This is a breadth-first search where the size of the frontier is
    constrainted to the beam width.

    :param model: RobustFill model.
    :param tokenizer: Tokenizes the input and output strings.
    :param width: Beam width to use for decoding.
    :param max_program_length: Limit on length of the programs to search.
    :param strings: List of input-output pairs.
    :returns: Top `width` programs and their scores.
    """
    with torch.no_grad():
        num_examples = len(strings)
        str_tokens = [
            (tokenizer.tokenize_string(input_),
             tokenizer.tokenize_string(output))
            for input_, output in strings
        ]
        hidden, all_hidden = model.encode([str_tokens])
        candidates = [(0, [], hidden)]

        for _ in range(max_program_length):
            new_cands = []
            for cand in candidates:
                log_prob, decoder_input, hidden = cand
                input_ = None
                if len(decoder_input) > 0:
                    prev = decoder_input[-1:]
                    if prev[0] == tokenizer.op_token_table[EOS]:
                        # Program already complete, don't need to decode
                        # the next token.
                        _push_cand(new_cands, width, cand)
                        continue

                    # Use the previous output as the input to the decoder.
                    input_ = F.one_hot(
                        torch.LongTensor(prev),
                        num_classes=len(tokenizer.op_token_table))
                    # We have to repeat the input for each example
                    # due to the max-pooling in the decoder.
                    input_ = input_.repeat(num_examples, 1)

                logits, new_hidden = model.program_decoder.decode(
                    input_=input_,
                    hidden=hidden,
                    output_all_hidden=all_hidden,
                    num_examples=num_examples,
                )

                _add_candidates(
                    tokenizer=tokenizer,
                    width=width,
                    log_prob=log_prob,
                    decoder_input=decoder_input,
                    new_hidden=new_hidden,
                    logits=logits,
                    new_cands=new_cands,
                )

            candidates = new_cands

        return candidates


def _push_cand(heap: List, width: int, cand: Tuple) -> None:
    """Add candidate to the heap, keeping only the top `width` candidates."""
    if len(heap) < width:
        heappush(heap, cand)
    else:
        heappushpop(heap, cand)


def _add_candidates(
        tokenizer: Tokenizer,
        width: int,
        log_prob: float,
        decoder_input: List[int],
        new_hidden: Tuple[torch.Tensor, torch.Tensor],
        logits: torch.Tensor,
        new_cands: List[Tuple]) -> None:
    """
    Add new candidates based on the decoder output `logits` to the
    frontier `new_cands`.

    :param tokenizer: Tokenizer used to parse programs in order to eliminate
        dead end candidates in the beam search.
    :param width: Beam width to use.
    :param log_prob: Log probability of the prefix used to generate the current
        candidates.
    :param decoder_input: Input used to generate the candidates.
    :param new_hidden: Hidden state of the decoder after generating
        the candidates.
    :param logits: Output generated by the decoder (before softmax).
    :param new_cands: Frontier (heap) to add the new candidates to.
    """
    added = 0
    i = 0
    sorted_log_probs, indices = torch.sort(
        F.log_softmax(logits.squeeze(0), dim=0),
        descending=True)
    while added < width and i < sorted_log_probs.size()[0]:
        # Update the frontier.
        token = indices[i].item()
        new_prog = decoder_input + [token]
        try:
            tokenizer.parse_program(new_prog)
        except (IndexError, TypeError, ValueError):
            # Discard invalid programs.
            i += 1
            continue
        nc = (
            log_prob + sorted_log_probs[i].item(),
            new_prog,
            new_hidden,
        )
        _push_cand(new_cands, width, nc)
        added += 1
        i += 1
